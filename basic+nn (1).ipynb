{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "    \n",
    "    Returns: \n",
    "    results -- the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, name = 'x')\n",
    "\n",
    "    sigmoid = tf.sigmoid(x)\n",
    "\n",
    "    with tf.Session() as sess: \n",
    "        result = sess.run(sigmoid, feed_dict = {x: z})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cost_mse(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the cost using the sigmoid cross entropy\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
    "    labels -- vector of labels y (1 or 0) \n",
    "    \n",
    "    Returns:\n",
    "    cost -- runs the session of the cost (formula (2))\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the placeholders for \"logits\" (z) and \"labels\" (y)\n",
    "    z = tf.placeholder(tf.float32, name = 'z')\n",
    "    y = tf.placeholder(tf.float32, name = 'y')\n",
    "    \n",
    "    # Use the loss function\n",
    "    cost = tf.nn.l2_loss(tf.subtract(z, y))\n",
    "    \n",
    "    # Create a session.\n",
    "    sess = tf.Session()\n",
    "    cost = sess.run(cost, feed_dict = {z: logits, y: labels})\n",
    "    \n",
    "    # Close the session.\n",
    "    sess.close()\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the cost using the sigmoid cross entropy\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
    "    labels -- vector of labels y (1 or 0) \n",
    "    \n",
    "    Returns:\n",
    "    cost -- runs the session of the cost (formula (2))\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the placeholders for \"logits\" (z) and \"labels\" (y)\n",
    "    z = tf.placeholder(tf.float32, name = 'z')\n",
    "    y = tf.placeholder(tf.float32, name = 'y')\n",
    "    \n",
    "    # Use the loss function\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = z,  labels = y)\n",
    "    \n",
    "    # Create a session.\n",
    "    sess = tf.Session()\n",
    "    cost = sess.run(cost, feed_dict = {z: logits, y: labels})\n",
    "    \n",
    "    # Close the session.\n",
    "    sess.close()\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'.\n",
    "    C = tf.constant(C, name='C')\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis\n",
    "    one_hot_matrix = tf.one_hot(labels, C, axis=0)\n",
    "    \n",
    "    # Create the session \n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session\n",
    "    sess.close()\n",
    "    \n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createword2Vec(lebels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    labels = \"train_array.csv\"\n",
    "    # embedding = get_dataset(labels).shuffle(1500).batch(1)\n",
    "    embedding = pd.read_csv(labels)\n",
    "    trainArray = np.array(embedding)\n",
    "    print(trainArray)\n",
    "    #now we want to find the \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'.\n",
    "    C = tf.constant(C, name='C')\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis\n",
    "    one_hot_matrix = tf.one_hot(labels, C, axis=0)\n",
    "    \n",
    "    # Create the session \n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session\n",
    "    sess.close()\n",
    "    \n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: ones\n",
    "\n",
    "def ones(shape):\n",
    "    \"\"\"\n",
    "    Creates an array of ones of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    shape -- shape of the array you want to create\n",
    "        \n",
    "    Returns: \n",
    "    ones -- array containing only ones\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create \"ones\" tensor using tf.ones(...). \n",
    "    ones = tf.ones(shape)\n",
    "    \n",
    "    # Create the session\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session to compute 'ones'\n",
    "    ones = sess.run(ones)\n",
    "    \n",
    "    # Close the session.\n",
    "    sess.close()\n",
    "\n",
    "    return ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"tf.float32\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"tf.float32\"\n",
    "    \n",
    "    Tips:\n",
    "    - Using None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape= [n_x, None], name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape= [n_y, None], name = \"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "        \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    \n",
    "    # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X), b1)                                              # Z1 = np.dot(W1, X) + b1\n",
    "    # A1 = tf.nn.relu(Z1)                                             # A1 = relu(Z1)\n",
    "    \n",
    "    return Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, keep_prob = 1.0):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X), b1)                                              # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                             # A1 = relu(Z1)\n",
    "    D1 = tf.nn.dropout(A1,keep_prob)\n",
    "    Z2 = tf.add(tf.matmul(W2,D1), b2)                                              # Z2 = np.dot(W2, D1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n",
    "    D2 = tf.nn.dropout(A2,keep_prob)\n",
    "    Z3 = tf.add(tf.matmul(W3,D2), b3)                                              # Z3 = np.dot(W3, D2) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost_squared(Z3,Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    cost = tf.nn.l2_loss(tf.subtract(labels, logits))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_regression_parameters(use_binary_classification = True, use_word2vec=False):\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    num_outputs = 1\n",
    "    if use_binary_classification:\n",
    "        num_outputs = 2\n",
    "    num_inputs = 510\n",
    "    if use_word2vec:\n",
    "        num_inputs = 210\n",
    "    W1 = tf.get_variable(\"W1\", [num_outputs,num_inputs], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [num_outputs,1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True, use_linear = False, keep_prob=1.0, \n",
    "          use_binary_classification=True, use_word2vec = False):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    if use_linear:\n",
    "        parameters = initialize_regression_parameters(use_binary_classification, use_word2vec)\n",
    "        Z3 = logistic_regression(X, parameters)\n",
    "    else: \n",
    "        parameters = initialize_parameters(use_binary_classification, use_word2vec)\n",
    "        Z3 = forward_propagation(X, parameters, keep_prob)\n",
    "    \n",
    "    if use_binary_classification:\n",
    "        cost = compute_cost(Z3, Y)\n",
    "    else:\n",
    "        cost = compute_cost_squared(Z3, Y)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per fives)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        if use_binary_classification:\n",
    "            correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "            print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        else:\n",
    "            correct_prediction = tf.nn.l2_loss(tf.subtract(Z3, Y))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            print (\"Train Cost:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "            print (\"Test Cost:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_linear = False\n",
    "keep_prob=1.0\n",
    "use_binary_classification=True\n",
    "use_word2vec = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 - Test with your own image (optional / ungraded exercise)\n",
    "\n",
    "Congratulations on finishing this assignment. You can now take a picture of your hand and see the output of your model. To do that:\n",
    "    1. Click on \"File\" in the upper bar of this notebook, then click \"Open\" to go on your Coursera Hub.\n",
    "    2. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n",
    "    3. Write your image's name in the following code\n",
    "    4. Run the code and check if the algorithm is right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0, 1]\n",
    "\n",
    "def get_dataset(file_path, **kwargs):\n",
    "    df = pd.read_csv(file_path)\n",
    "    target = df.pop('0')\n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices((df.values, target.values))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "train_dataset= get_dataset(\"trian_set.csv\").shuffle(1500).batch(1)\n",
    "# X_test_orig, Y_test_orig = get_dataset(\"test_set.csv\").shuffle(359).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"trian_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0        1        2        3        4        5        6        7  \\\n",
      "0     0.0  2681.02  2680.88  2680.89  2680.92  2680.58  2680.37  2680.40   \n",
      "1     1.0  2554.23  2554.25  2554.34  2554.01  2553.90  2554.16  2554.16   \n",
      "2     1.0  2494.45  2494.00  2494.42  2494.91  2494.96  2495.07  2495.03   \n",
      "3     0.0  2506.70  2507.13  2506.72  2506.84  2506.97  2506.74  2506.54   \n",
      "4     1.0  2890.07  2890.37  2890.48  2890.54  2891.89  2890.97  2889.70   \n",
      "5     0.0  2747.50  2747.67  2747.69  2747.66  2747.65  2747.73  2747.40   \n",
      "6     1.0  2783.66  2783.28  2783.52  2783.94  2782.92  2782.14  2781.31   \n",
      "7     1.0  2381.08  2381.02  2381.09  2381.06  2381.27  2381.41  2381.35   \n",
      "8     1.0  2746.22  2746.59  2746.48  2746.67  2746.72  2747.07  2746.74   \n",
      "9     1.0  2750.57  2751.21  2752.31  2751.78  2751.15  2750.62  2750.08   \n",
      "10    0.0  2792.43  2793.48  2792.93  2791.90  2790.30  2790.82  2790.41   \n",
      "11    0.0  2443.27  2443.56  2443.61  2442.58  2443.03  2443.92  2444.39   \n",
      "12    1.0  2812.72  2812.72  2812.49  2812.36  2811.56  2810.81  2811.27   \n",
      "13    0.0  2787.93  2788.16  2788.30  2787.66  2788.28  2787.38  2786.53   \n",
      "14    0.0  2683.56  2682.22  2683.96  2684.74  2684.24  2683.34  2682.49   \n",
      "15    0.0  2776.30  2774.88  2774.26  2774.84  2774.21  2774.47  2772.49   \n",
      "16    1.0  2826.31  2826.63  2826.60  2826.24  2826.35  2826.61  2826.09   \n",
      "17    1.0  2754.68  2755.38  2757.19  2757.86  2757.55  2757.46  2756.71   \n",
      "18    0.0  2634.19  2634.95  2633.80  2633.69  2633.81  2634.24  2634.66   \n",
      "19    0.0  2885.02  2885.26  2884.97  2884.50  2883.90  2884.92  2885.46   \n",
      "20    1.0  2881.46  2881.75  2882.36  2882.03  2880.44  2880.46  2880.21   \n",
      "21    1.0  2576.89  2576.85  2577.13  2577.06  2577.05  2576.85  2577.04   \n",
      "22    1.0  2525.47  2525.23  2525.54  2525.48  2525.71  2525.36  2525.06   \n",
      "23    1.0  2456.73  2456.85  2456.89  2456.65  2456.81  2456.83  2457.41   \n",
      "24    0.0  2307.85  2307.61  2307.80  2307.82  2307.95  2307.91  2308.30   \n",
      "25    1.0  2465.70  2465.89  2466.41  2466.47  2466.75  2466.83  2467.81   \n",
      "26    1.0  2756.95  2758.13  2757.20  2755.70  2756.82  2757.42  2757.70   \n",
      "27    0.0  2739.64  2741.55  2741.24  2741.93  2742.16  2741.79  2742.08   \n",
      "28    1.0  2732.50  2731.34  2731.15  2729.23  2729.86  2730.77  2731.20   \n",
      "29    0.0  2546.30  2546.26  2546.99  2546.19  2545.61  2545.31  2545.91   \n",
      "...   ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "1470  0.0  2475.36  2475.62  2475.69  2475.53  2475.49  2475.54  2475.75   \n",
      "1471  0.0  2779.87  2779.77  2780.31  2780.48  2780.22  2779.90  2779.27   \n",
      "1472  1.0  2712.46  2711.89  2712.48  2712.41  2711.93  2711.76  2711.20   \n",
      "1473  0.0  2883.82  2883.91  2882.69  2882.12  2880.57  2880.64  2879.92   \n",
      "1474  0.0  2895.91  2895.71  2896.36  2896.24  2896.34  2896.65  2896.55   \n",
      "1475  0.0  2815.69  2815.51  2816.00  2816.67  2818.52  2817.81  2816.49   \n",
      "1476  0.0  2694.21  2695.26  2694.42  2692.63  2691.83  2692.19  2690.39   \n",
      "1477  1.0  2787.86  2788.18  2787.89  2788.30  2788.56  2789.65  2789.20   \n",
      "1478  1.0  2783.45  2783.20  2783.37  2782.50  2782.64  2782.60  2782.52   \n",
      "1479  1.0  2462.88  2463.34  2463.55  2463.77  2463.79  2463.92  2463.91   \n",
      "1480  0.0  2651.56  2650.07  2650.54  2649.33  2648.54  2648.18  2648.86   \n",
      "1481  1.0  2248.96  2249.67  2249.29  2249.09  2248.97  2249.44  2249.38   \n",
      "1482  1.0  2576.78  2574.47  2573.77  2573.63  2573.40  2573.47  2571.83   \n",
      "1483  0.0  2715.98  2715.33  2715.34  2715.68  2717.68  2715.77  2714.89   \n",
      "1484  0.0  2868.18  2866.91  2867.51  2867.55  2867.52  2867.24  2867.63   \n",
      "1485  1.0  2474.74  2474.76  2474.88  2474.63  2474.89  2474.86  2474.72   \n",
      "1486  0.0  2755.34  2757.04  2756.88  2757.55  2757.68  2757.82  2758.97   \n",
      "1487  1.0  2682.38  2679.33  2679.11  2679.48  2678.42  2677.96  2677.70   \n",
      "1488  1.0  2445.90  2445.80  2445.62  2445.34  2445.63  2445.88  2445.99   \n",
      "1489  0.0  2479.00  2478.83  2478.65  2478.55  2478.50  2478.32  2478.30   \n",
      "1490  0.0  2778.54  2777.56  2778.03  2778.48  2778.98  2778.80  2777.85   \n",
      "1491  0.0  2673.80  2674.61  2674.16  2673.52  2672.42  2672.45  2672.76   \n",
      "1492  1.0  2779.24  2779.32  2779.49  2779.30  2779.36  2778.75  2779.76   \n",
      "1493  1.0  2853.31  2853.06  2852.97  2853.35  2853.05  2854.16  2854.29   \n",
      "1494  1.0  2369.27  2369.01  2368.86  2369.16  2369.43  2369.18  2368.94   \n",
      "1495  1.0  2912.99  2912.52  2912.72  2912.93  2912.69  2912.74  2912.78   \n",
      "1496  1.0  2500.24  2500.20  2499.29  2499.36  2499.48  2499.56  2497.70   \n",
      "1497  0.0  2758.06  2757.86  2758.32  2758.96  2759.38  2759.32  2759.49   \n",
      "1498  0.0  2266.62  2266.53  2267.00  2266.72  2266.58  2266.57  2266.58   \n",
      "1499  0.0  2361.49  2361.56  2361.70  2361.89  2362.26  2362.04  2361.94   \n",
      "\n",
      "            8        9 ...   501  502  503  504  505  506  507  508  509  510  \n",
      "0     2680.45  2680.45 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1     2554.14  2553.81 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2     2495.34  2495.53 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3     2506.47  2506.51 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4     2888.23  2887.50 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "5     2747.37  2747.34 ...   0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  \n",
      "6     2781.34  2782.53 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "7     2381.56  2381.37 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "8     2746.69  2746.17 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "9     2748.22  2747.89 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "10    2790.40  2789.83 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "11    2444.45  2444.25 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "12    2810.89  2811.35 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "13    2785.89  2784.91 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "14    2685.04  2683.42 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  \n",
      "15    2772.60  2771.53 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "16    2826.11  2825.03 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "17    2756.20  2756.36 ...   0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "18    2634.38  2632.96 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "19    2883.61  2886.02 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "20    2879.88  2880.40 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "21    2577.22  2576.92 ...   1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "22    2525.04  2524.49 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "23    2457.30  2456.97 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "24    2308.27  2308.38 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "25    2468.04  2468.09 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "26    2757.98  2758.90 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "27    2742.08  2742.26 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "28    2730.21  2728.40 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "29    2545.39  2545.34 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...       ...      ... ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "1470  2475.73  2475.98 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1471  2779.09  2778.87 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1472  2712.02  2711.07 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1473  2880.50  2879.41 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1474  2896.71  2897.30 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1475  2816.93  2817.36 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  3.0  0.0  \n",
      "1476  2691.37  2690.79 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1477  2788.83  2787.01 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1478  2782.71  2780.94 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1479  2464.39  2464.33 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1480  2647.65  2647.82 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1481  2249.75  2250.62 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1482  2571.59  2570.12 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1483  2714.20  2713.20 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1484  2867.88  2867.71 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1485  2474.66  2474.49 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1486  2759.40  2760.25 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1487  2678.45  2677.40 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1488  2446.20  2446.19 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1489  2478.18  2477.80 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1490  2777.73  2777.78 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1491  2672.29  2672.87 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1492  2779.95  2779.58 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1493  2853.75  2854.31 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1494  2368.78  2369.11 ...   0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1495  2912.80  2912.88 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1496  2496.87  2497.12 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1497  2759.48  2759.99 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1498  2266.75  2266.75 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1499  2362.24  2362.45 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[1500 rows x 511 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.01517522e+02   7.03356449e+02   7.04275912e+02 ...,   9.45902000e-01\n",
      "    2.35500000e-01  -6.03433000e-01]\n",
      " [  7.85955151e+01   7.51127900e+01   7.56118499e+01 ...,   8.05732000e-01\n",
      "   -1.58547700e+00  -4.92757300e+00]\n",
      " [  1.96888998e+02   1.97767893e+02   1.97804514e+02 ...,   5.83068000e-01\n",
      "   -8.63542000e-01  -3.35342300e+00]\n",
      " ..., \n",
      " [  6.47062547e+02   6.45307351e+02   6.45284256e+02 ...,   6.59360100e+00\n",
      "   -4.46080200e+00  -3.42585000e-01]\n",
      " [  8.94689130e+02   8.90568377e+02   8.90983646e+02 ...,   2.74875100e+00\n",
      "   -1.61028900e+00   1.68555500e+00]\n",
      " [  1.41457014e+03   1.41477272e+03   1.41573497e+03 ...,   5.85406000e-01\n",
      "   -2.54692800e+00  -6.85100000e+00]]\n",
      "(1500, 210)\n",
      "(1, 1500)\n"
     ]
    }
   ],
   "source": [
    "training_set = \"trian_set_mse.csv\"\n",
    "test_set = \"test_set_mse.csv\"\n",
    "numTestSamples = 359\n",
    "if use_word2vec:\n",
    "    numTestSamples = 621\n",
    "if use_binary_classification:\n",
    "    training_set = \"trian_set.csv\"\n",
    "    test_set = \"test_set.csv\"\n",
    "if use_word2vec:\n",
    "    training_set = \"total_array_train.csv\" #input\n",
    "    test_set = \"total_array_test.csv\" #input\n",
    "train_dataset= get_dataset(training_set).shuffle(1500).batch(1)\n",
    "train = pd.read_csv(training_set)\n",
    "trainArray = np.array(train)\n",
    "variances = np.var(trainArray[:, 1:10], axis=1)\n",
    "std = np.sqrt(variances)\n",
    "mean = np.sum(trainArray[:, 1:10], axis=1)/10\n",
    "mean = mean.reshape(1500,1)\n",
    "std = std.reshape(1500,1)\n",
    "trainArray[:, :10] = (trainArray[:, :10] - mean)/std\n",
    "X_train_orig = trainArray[:, 1:]\n",
    "print(trainArray)\n",
    "Y_train_orig = trainArray[:, 0].reshape(1,1500).astype(float)\n",
    "if use_binary_classification:\n",
    "    Y_train_orig = trainArray[:, 0].reshape(1,1500).astype(int)\n",
    "if use_word2vec:\n",
    "    Y_train_orig = (trainArray[:, 0] > trainArray[:, 1]).reshape(1,1500).astype(int)\n",
    "if use_word2vec and not use_binary_classification:\n",
    "    Y_train_orig = (trainArray[:, 0] - trainArray[:, 1]).reshape(1,1500).astype(float)\n",
    "test = pd.read_csv(test_set)\n",
    "testArray = np.array(test)\n",
    "variances = np.var(testArray[:, 1:10], axis=1)\n",
    "std = np.sqrt(variances)\n",
    "mean = np.sum(testArray[:, 1:10], axis=1)/10\n",
    "mean = mean.reshape(numTestSamples,1)\n",
    "std = std.reshape(numTestSamples,1)\n",
    "testArray[:, :10] = (testArray[:, :10] - mean)/std\n",
    "X_test_orig = testArray[:, 1:]\n",
    "Y_test_orig = testArray[:, 0].reshape(1,numTestSamples).astype(float)\n",
    "if use_binary_classification:\n",
    "    Y_test_orig = testArray[:, 0].reshape(1,numTestSamples).astype(int)\n",
    "if use_word2vec:\n",
    "    Y_test_orig = (testArray[:, 0] > testArray[:, 1]).reshape(1,numTestSamples).astype(int)\n",
    "if use_word2vec and not use_binary_classification:\n",
    "    Y_test_orig = (testArray[:, 0] - testArray[:, 1]).reshape(1,numTestSamples).astype(float)\n",
    "print(X_train_orig.shape)\n",
    "print(Y_train_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1500\n",
      "number of test examples = 621\n",
      "X_train shape: (210, 1500)\n",
      "Y_train shape: (2, 1500)\n",
      "X_test shape: (210, 621)\n",
      "Y_test shape: (2, 621)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the training and test images\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "# Normalize image vectors\n",
    "X_train = X_train_flatten\n",
    "X_test = X_test_flatten\n",
    "# Convert training and test labels to one hot matrices\n",
    "if use_binary_classification:\n",
    "    Y_train = convert_to_one_hot(Y_train_orig,2)\n",
    "    Y_test = convert_to_one_hot(Y_test_orig,2)\n",
    "else:\n",
    "    Y_train = Y_train_orig\n",
    "    Y_test = Y_test_orig\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X_2:0\", shape=(210, ?), dtype=float32)\n",
      "Y = Tensor(\"Y_2:0\", shape=(2, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(510, 1)\n",
    "if use_binary_classification:\n",
    "    X, Y = create_placeholders(510, 2)\n",
    "if use_word2vec:\n",
    "    X, Y = create_placeholders(210, 2)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(use_binary_classification=True, use_word2vec=False):\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    num_outputs = 1\n",
    "    num_inputs = 510\n",
    "    if use_binary_classification:\n",
    "        num_outputs = 2\n",
    "    if use_word2vec:\n",
    "        num_inputs = 210\n",
    "        \n",
    "    W1 = tf.get_variable(\"W1\", [25,num_inputs], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [num_outputs,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [num_outputs,1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 84.278019\n",
      "Cost after epoch 100: 0.678322\n",
      "Cost after epoch 200: 0.628948\n",
      "Cost after epoch 300: 0.602756\n",
      "Cost after epoch 400: 0.600136\n",
      "Cost after epoch 500: 0.568606\n",
      "Cost after epoch 600: 0.549732\n",
      "Cost after epoch 700: 0.542183\n",
      "Cost after epoch 800: 0.557238\n",
      "Cost after epoch 900: 0.503493\n",
      "Cost after epoch 1000: 0.586134\n",
      "Cost after epoch 1100: 0.506449\n",
      "Cost after epoch 1200: 0.465861\n",
      "Cost after epoch 1300: 0.463671\n",
      "Cost after epoch 1400: 0.459083\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc50d17c908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.776667\n",
      "Test Accuracy: 0.499195\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test, use_linear=use_linear, keep_prob=keep_prob, use_binary_classification=use_binary_classification, use_word2vec = use_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_dataset = get_dataset(\"tweets.csv\").shuffle(len(df)).batch(1)\n",
    "def get_compiled_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f239e5814793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_compiled_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# model.fit(train_dataset, epochs=15)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdict_slices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_slices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-333753a0511c>\u001b[0m in \u001b[0;36mget_compiled_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train_dataset = get_dataset(\"tweets.csv\").shuffle(len(df)).batch(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_compiled_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     model = tf.keras.Sequential([\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'keras'"
     ]
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "# model.fit(train_dataset, epochs=15)\n",
    "dict_slices = tf.data.Dataset.from_tensor_slices((df.to_dict('list'), target.values)).batch(16)\n",
    "model_func.fit(dict_slices, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_csv(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, csv_row = reader.read(filename_queue)\n",
    "    record_defaults = [[0],[0],[0],[0],[0]]\n",
    "    colHour,colQuarter,colAction,colUser,colLabel = tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "    features = tf.stack([colHour,colQuarter,colAction,colUser])  \n",
    "    label = tf.stack([colLabel])  \n",
    "    return features, label\n",
    "\n",
    "filenames = [\"tweets.csv\"]\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(filenames, num_epochs=1, shuffle=False)\n",
    "example, country = create_file_reader_ops(filename_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file_path = \"tweets.csv\"\n",
    "test_file_path = \"tweets.csv\"\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "## START CODE HERE ## (PUT YOUR IMAGE NAME) \n",
    "my_file = \"tweets.csv\"\n",
    "## END CODE HERE ##\n",
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(my_file) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            print(f'\\t{row[0]} works in the {row[1]} department, and was born in {row[2]}.')\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} tweets.')\n",
    "    my_training_set = predict()\n",
    "    # Loading the dataset\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset() #TODO: change this so that it works\n",
    "\n",
    "# We preprocess your image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "image = image/255.\n",
    "my_image = scipy.misc.imresize(image, size=(64,64)).reshape((1, 64*64*3)).T\n",
    "my_image_prediction = predict(my_image, parameters)\n",
    "\n",
    "plt.imshow(image)\n",
    "print(\"Your algorithm predicts: y = \" + str(np.squeeze(my_image_prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
