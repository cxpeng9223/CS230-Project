{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "    \n",
    "    Returns: \n",
    "    results -- the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, name = 'x')\n",
    "\n",
    "    sigmoid = tf.sigmoid(x)\n",
    "    with tf.Session() as sess: \n",
    "        result = sess.run(sigmoid, feed_dict = {x: z})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the cost using the sigmoid cross entropy\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
    "    labels -- vector of labels y (1 or 0) \n",
    "    \n",
    "    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n",
    "    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n",
    "    \n",
    "    Returns:\n",
    "    cost -- runs the session of the cost (formula (2))\n",
    "    \"\"\"\n",
    "    \n",
    "    z = tf.placeholder(tf.float32, name = 'z')\n",
    "    y = tf.placeholder(tf.float32, name = 'y')\n",
    "    \n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = z,  labels = y)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "\n",
    "    cost = sess.run(cost, feed_dict = {z: logits, y: labels})\n",
    "    \n",
    "    sess.close()\n",
    "     \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    C = tf.constant(C, name='C')\n",
    "    \n",
    "    one_hot_matrix = tf.one_hot(labels, C, axis=0)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    \n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "\n",
    "    sess.close()   \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ones(shape):\n",
    "    \"\"\"\n",
    "    Creates an array of ones of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    shape -- shape of the array you want to create\n",
    "        \n",
    "    Returns: \n",
    "    ones -- array containing only ones\n",
    "    \"\"\"\n",
    "    ones = tf.ones(shape)\n",
    "    sess = tf.Session()\n",
    "    ones = sess.run(ones)\n",
    "\n",
    "    sess.close()\n",
    "    return ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"tf.float32\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"tf.float32\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape= [n_x, None], name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape= [n_y, None], name = \"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1,X), b1)                                         \n",
    "    A1 = tf.nn.relu(Z1)                                           \n",
    "    Z2 = tf.add(tf.matmul(W2,A1), b2)                             \n",
    "    A2 = tf.nn.relu(Z2)                                          \n",
    "    Z3 = tf.add(tf.matmul(W3,A2), b3)                                    \n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                                                                             \n",
    "    (n_x, m) = X_train.shape                  \n",
    "    n_y = Y_train.shape[0]                           \n",
    "    costs = []                                       \n",
    "    \n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    \n",
    "    cost = compute_cost(Z3, Y)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                     \n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per fives)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"trian_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0        1        2        3        4        5        6        7  \\\n",
      "0     0.0  2681.02  2680.88  2680.89  2680.92  2680.58  2680.37  2680.40   \n",
      "1     1.0  2554.23  2554.25  2554.34  2554.01  2553.90  2554.16  2554.16   \n",
      "2     1.0  2494.45  2494.00  2494.42  2494.91  2494.96  2495.07  2495.03   \n",
      "3     0.0  2506.70  2507.13  2506.72  2506.84  2506.97  2506.74  2506.54   \n",
      "4     1.0  2890.07  2890.37  2890.48  2890.54  2891.89  2890.97  2889.70   \n",
      "5     0.0  2747.50  2747.67  2747.69  2747.66  2747.65  2747.73  2747.40   \n",
      "6     1.0  2783.66  2783.28  2783.52  2783.94  2782.92  2782.14  2781.31   \n",
      "7     1.0  2381.08  2381.02  2381.09  2381.06  2381.27  2381.41  2381.35   \n",
      "8     1.0  2746.22  2746.59  2746.48  2746.67  2746.72  2747.07  2746.74   \n",
      "9     1.0  2750.57  2751.21  2752.31  2751.78  2751.15  2750.62  2750.08   \n",
      "10    0.0  2792.43  2793.48  2792.93  2791.90  2790.30  2790.82  2790.41   \n",
      "11    0.0  2443.27  2443.56  2443.61  2442.58  2443.03  2443.92  2444.39   \n",
      "12    1.0  2812.72  2812.72  2812.49  2812.36  2811.56  2810.81  2811.27   \n",
      "13    0.0  2787.93  2788.16  2788.30  2787.66  2788.28  2787.38  2786.53   \n",
      "14    0.0  2683.56  2682.22  2683.96  2684.74  2684.24  2683.34  2682.49   \n",
      "15    0.0  2776.30  2774.88  2774.26  2774.84  2774.21  2774.47  2772.49   \n",
      "16    1.0  2826.31  2826.63  2826.60  2826.24  2826.35  2826.61  2826.09   \n",
      "17    1.0  2754.68  2755.38  2757.19  2757.86  2757.55  2757.46  2756.71   \n",
      "18    0.0  2634.19  2634.95  2633.80  2633.69  2633.81  2634.24  2634.66   \n",
      "19    0.0  2885.02  2885.26  2884.97  2884.50  2883.90  2884.92  2885.46   \n",
      "20    1.0  2881.46  2881.75  2882.36  2882.03  2880.44  2880.46  2880.21   \n",
      "21    1.0  2576.89  2576.85  2577.13  2577.06  2577.05  2576.85  2577.04   \n",
      "22    1.0  2525.47  2525.23  2525.54  2525.48  2525.71  2525.36  2525.06   \n",
      "23    1.0  2456.73  2456.85  2456.89  2456.65  2456.81  2456.83  2457.41   \n",
      "24    0.0  2307.85  2307.61  2307.80  2307.82  2307.95  2307.91  2308.30   \n",
      "25    1.0  2465.70  2465.89  2466.41  2466.47  2466.75  2466.83  2467.81   \n",
      "26    1.0  2756.95  2758.13  2757.20  2755.70  2756.82  2757.42  2757.70   \n",
      "27    0.0  2739.64  2741.55  2741.24  2741.93  2742.16  2741.79  2742.08   \n",
      "28    1.0  2732.50  2731.34  2731.15  2729.23  2729.86  2730.77  2731.20   \n",
      "29    0.0  2546.30  2546.26  2546.99  2546.19  2545.61  2545.31  2545.91   \n",
      "...   ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "1470  0.0  2475.36  2475.62  2475.69  2475.53  2475.49  2475.54  2475.75   \n",
      "1471  0.0  2779.87  2779.77  2780.31  2780.48  2780.22  2779.90  2779.27   \n",
      "1472  1.0  2712.46  2711.89  2712.48  2712.41  2711.93  2711.76  2711.20   \n",
      "1473  0.0  2883.82  2883.91  2882.69  2882.12  2880.57  2880.64  2879.92   \n",
      "1474  0.0  2895.91  2895.71  2896.36  2896.24  2896.34  2896.65  2896.55   \n",
      "1475  0.0  2815.69  2815.51  2816.00  2816.67  2818.52  2817.81  2816.49   \n",
      "1476  0.0  2694.21  2695.26  2694.42  2692.63  2691.83  2692.19  2690.39   \n",
      "1477  1.0  2787.86  2788.18  2787.89  2788.30  2788.56  2789.65  2789.20   \n",
      "1478  1.0  2783.45  2783.20  2783.37  2782.50  2782.64  2782.60  2782.52   \n",
      "1479  1.0  2462.88  2463.34  2463.55  2463.77  2463.79  2463.92  2463.91   \n",
      "1480  0.0  2651.56  2650.07  2650.54  2649.33  2648.54  2648.18  2648.86   \n",
      "1481  1.0  2248.96  2249.67  2249.29  2249.09  2248.97  2249.44  2249.38   \n",
      "1482  1.0  2576.78  2574.47  2573.77  2573.63  2573.40  2573.47  2571.83   \n",
      "1483  0.0  2715.98  2715.33  2715.34  2715.68  2717.68  2715.77  2714.89   \n",
      "1484  0.0  2868.18  2866.91  2867.51  2867.55  2867.52  2867.24  2867.63   \n",
      "1485  1.0  2474.74  2474.76  2474.88  2474.63  2474.89  2474.86  2474.72   \n",
      "1486  0.0  2755.34  2757.04  2756.88  2757.55  2757.68  2757.82  2758.97   \n",
      "1487  1.0  2682.38  2679.33  2679.11  2679.48  2678.42  2677.96  2677.70   \n",
      "1488  1.0  2445.90  2445.80  2445.62  2445.34  2445.63  2445.88  2445.99   \n",
      "1489  0.0  2479.00  2478.83  2478.65  2478.55  2478.50  2478.32  2478.30   \n",
      "1490  0.0  2778.54  2777.56  2778.03  2778.48  2778.98  2778.80  2777.85   \n",
      "1491  0.0  2673.80  2674.61  2674.16  2673.52  2672.42  2672.45  2672.76   \n",
      "1492  1.0  2779.24  2779.32  2779.49  2779.30  2779.36  2778.75  2779.76   \n",
      "1493  1.0  2853.31  2853.06  2852.97  2853.35  2853.05  2854.16  2854.29   \n",
      "1494  1.0  2369.27  2369.01  2368.86  2369.16  2369.43  2369.18  2368.94   \n",
      "1495  1.0  2912.99  2912.52  2912.72  2912.93  2912.69  2912.74  2912.78   \n",
      "1496  1.0  2500.24  2500.20  2499.29  2499.36  2499.48  2499.56  2497.70   \n",
      "1497  0.0  2758.06  2757.86  2758.32  2758.96  2759.38  2759.32  2759.49   \n",
      "1498  0.0  2266.62  2266.53  2267.00  2266.72  2266.58  2266.57  2266.58   \n",
      "1499  0.0  2361.49  2361.56  2361.70  2361.89  2362.26  2362.04  2361.94   \n",
      "\n",
      "            8        9 ...   501  502  503  504  505  506  507  508  509  510  \n",
      "0     2680.45  2680.45 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1     2554.14  2553.81 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2     2495.34  2495.53 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3     2506.47  2506.51 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4     2888.23  2887.50 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "5     2747.37  2747.34 ...   0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  \n",
      "6     2781.34  2782.53 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "7     2381.56  2381.37 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "8     2746.69  2746.17 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "9     2748.22  2747.89 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "10    2790.40  2789.83 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "11    2444.45  2444.25 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "12    2810.89  2811.35 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "13    2785.89  2784.91 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "14    2685.04  2683.42 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  \n",
      "15    2772.60  2771.53 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "16    2826.11  2825.03 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "17    2756.20  2756.36 ...   0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "18    2634.38  2632.96 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "19    2883.61  2886.02 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "20    2879.88  2880.40 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "21    2577.22  2576.92 ...   1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "22    2525.04  2524.49 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "23    2457.30  2456.97 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "24    2308.27  2308.38 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "25    2468.04  2468.09 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "26    2757.98  2758.90 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "27    2742.08  2742.26 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "28    2730.21  2728.40 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "29    2545.39  2545.34 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...       ...      ... ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "1470  2475.73  2475.98 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1471  2779.09  2778.87 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1472  2712.02  2711.07 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1473  2880.50  2879.41 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1474  2896.71  2897.30 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1475  2816.93  2817.36 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  3.0  0.0  \n",
      "1476  2691.37  2690.79 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1477  2788.83  2787.01 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1478  2782.71  2780.94 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1479  2464.39  2464.33 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1480  2647.65  2647.82 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1481  2249.75  2250.62 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1482  2571.59  2570.12 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1483  2714.20  2713.20 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1484  2867.88  2867.71 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1485  2474.66  2474.49 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1486  2759.40  2760.25 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1487  2678.45  2677.40 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1488  2446.20  2446.19 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1489  2478.18  2477.80 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1490  2777.73  2777.78 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1491  2672.29  2672.87 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1492  2779.95  2779.58 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1493  2853.75  2854.31 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1494  2368.78  2369.11 ...   0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1495  2912.80  2912.88 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1496  2496.87  2497.12 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1497  2759.48  2759.99 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1498  2266.75  2266.75 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1499  2362.24  2362.45 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[1500 rows x 511 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   2.66057000e+03   2.65937000e+03 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   2.93324000e+03   2.93307000e+03 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   2.70375000e+03   2.70358000e+03 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " ..., \n",
      " [  1.00000000e+00   2.57476000e+03   2.57478000e+03 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   2.67703000e+03   2.67707000e+03 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   2.29411000e+03   2.29446000e+03 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "(1500, 5979)\n",
      "(1, 1500)\n"
     ]
    }
   ],
   "source": [
    "trainArray = np.array(train)\n",
    "print(trainArray)\n",
    "X_train_orig = trainArray[:, 1:]\n",
    "Y_train_orig = trainArray[:, 0].reshape(1,1500).astype(int)\n",
    "test = pd.read_csv(\"test_set.csv\")\n",
    "testArray = np.array(test)\n",
    "X_test_orig = testArray[:, 1:]\n",
    "Y_test_orig = testArray[:, 0].reshape(1,359).astype(int)\n",
    "print(X_train_orig.shape)\n",
    "print(Y_train_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1500\n",
      "number of test examples = 359\n",
      "X_train shape: (5979, 1500)\n",
      "Y_train shape: (2, 1500)\n",
      "X_test shape: (5979, 359)\n",
      "Y_test shape: (2, 359)\n"
     ]
    }
   ],
   "source": [
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "X_train = X_train_flatten\n",
    "X_test = X_test_flatten\n",
    "Y_train = convert_to_one_hot(Y_train_orig,2)\n",
    "Y_test = convert_to_one_hot(Y_test_orig,2)\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X_1:0\", shape=(5979, ?), dtype=float32)\n",
      "Y = Tensor(\"Y_1:0\", shape=(2, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(5979, 2)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    W1 = tf.get_variable(\"W1\", [25,5979], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [2,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [2,1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 9.289794\n",
      "Cost after epoch 100: 0.483114\n",
      "Cost after epoch 200: 0.354324\n",
      "Cost after epoch 300: 0.206277\n",
      "Cost after epoch 400: 0.147042\n",
      "Cost after epoch 500: 0.090229\n",
      "Cost after epoch 600: 0.094887\n",
      "Cost after epoch 700: 0.063658\n",
      "Cost after epoch 800: 0.038117\n",
      "Cost after epoch 900: 0.029079\n",
      "Cost after epoch 1000: 0.031158\n",
      "Cost after epoch 1100: 0.035730\n",
      "Cost after epoch 1200: 0.031199\n",
      "Cost after epoch 1300: 0.025991\n",
      "Cost after epoch 1400: 0.049639\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b26e2b780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.964\n",
      "Test Accuracy: 0.534819\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
